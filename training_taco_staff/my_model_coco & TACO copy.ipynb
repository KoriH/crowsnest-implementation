{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eab0d58",
   "metadata": {},
   "source": [
    "TACOdataset:\n",
    "\n",
    "https://www.kaggle.com/datasets/kneroma/tacotrashdataset?select=meta_df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#START HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88f1cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.1.46 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.43 ðŸš€ Python-3.11.5 torch-2.2.1 CPU (Apple M2 Max)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov5nu.pt, data=/Users/Kety/Desktop/archive 12.42.03 PM/data.yaml, epochs=10, time=None, patience=100, batch=8, imgsz=416, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
      "Overriding model.yaml nc=80 with nc=101\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1760  ultralytics.nn.modules.conv.Conv             [3, 16, 6, 2, 2]              \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      4800  ultralytics.nn.modules.block.C3              [32, 32, 1]                   \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     29184  ultralytics.nn.modules.block.C3              [64, 64, 2]                   \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  3    156928  ultralytics.nn.modules.block.C3              [128, 128, 3]                 \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1]                 \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1     90880  ultralytics.nn.modules.block.C3              [256, 128, 1, False]          \n",
      " 14                  -1  1      8320  ultralytics.nn.modules.conv.Conv             [128, 64, 1, 1]               \n",
      " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     22912  ultralytics.nn.modules.block.C3              [128, 64, 1, False]           \n",
      " 18                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1     74496  ultralytics.nn.modules.block.C3              [128, 128, 1, False]          \n",
      " 21                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 23                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1, False]          \n",
      " 24        [17, 20, 23]  1   1086907  ultralytics.nn.modules.head.Detect           [101, [64, 128, 256]]         \n",
      "YOLOv5n summary: 262 layers, 2844059 parameters, 2844043 gradients, 8.7 GFLOPs\n",
      "\n",
      "Transferred 391/427 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
      "Freezing layer 'model.24.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/Kety/Desktop/archive 12.42.03 PM/train/labels.cache... 42\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/Kety/Desktop/archive 12.42.03 PM/valid/labels.cache... 1715\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=9.5e-05, momentum=0.9) with parameter groups 69 weight(decay=0.0), 76 weight(decay=0.0005), 75 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kety/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G      1.388       5.15      1.206          1        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866      0.371      0.097     0.0296     0.0208\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G      1.382       3.77      1.177          3        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866      0.205      0.144     0.0599     0.0407\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G      1.336      3.352      1.151          1        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866       0.21      0.225     0.0848     0.0606\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.315      3.141      1.132          3        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866      0.167       0.23     0.0926     0.0672\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G      1.276      2.994      1.116          1        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866      0.179      0.243      0.105     0.0758\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G       1.26      2.885      1.103          4        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866      0.194      0.256      0.123     0.0889\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G      1.255      2.822      1.094          1        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866        0.2      0.257      0.127     0.0908\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G      1.247      2.736      1.089          4        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866       0.21      0.258      0.135     0.0978\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G      1.235      2.688      1.082          2        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866      0.238      0.255      0.141      0.101\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G      1.221      2.659      1.077          3        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866      0.237      0.256      0.143      0.104\n",
      "\n",
      "10 epochs completed in 1.598 hours.\n",
      "Optimizer stripped from runs/detect/train2/weights/last.pt, 5.9MB\n",
      "Optimizer stripped from runs/detect/train2/weights/best.pt, 5.9MB\n",
      "\n",
      "Validating runs/detect/train2/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.43 ðŸš€ Python-3.11.5 torch-2.2.1 CPU (Apple M2 Max)\n",
      "YOLOv5n summary (fused): 193 layers, 2838323 parameters, 0 gradients, 8.6 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866      0.237      0.256      0.144      0.104\n",
      "        Aluminium foil       1715         62      0.397      0.161      0.205      0.161\n",
      "            Bottle cap       1715        459      0.333      0.484      0.336      0.247\n",
      "                Bottle       1715        320      0.379      0.266      0.242       0.17\n",
      "          Broken glass       1715        123          1          0   0.000549   0.000112\n",
      "                   Can       1715        267      0.315      0.509      0.324      0.225\n",
      "                Carton       1715        263      0.248      0.384       0.24      0.179\n",
      "             Cigarette       1715        565          0          0    0.00778    0.00334\n",
      "                   Cup       1715        186      0.185      0.285      0.151       0.11\n",
      "                   Lid       1715         93        0.2      0.199      0.131      0.109\n",
      "          Other litter       1715        178      0.145     0.0674     0.0596     0.0418\n",
      "         Other plastic       1715        265      0.251     0.0377     0.0484     0.0312\n",
      "                 Paper       1715        178     0.0679     0.0127     0.0342     0.0265\n",
      " Plastic bag - wrapper       1715        854      0.308      0.389      0.257      0.181\n",
      "     Plastic container       1715         90      0.232      0.133      0.147        0.1\n",
      "               Pop tab       1715        125          0          0     0.0103    0.00614\n",
      "                 Straw       1715        120      0.154      0.192     0.0922     0.0685\n",
      "       Styrofoam piece       1715        113      0.169      0.168      0.116      0.099\n",
      "      Unlabeled litter       1715        569      0.167     0.0176     0.0338     0.0152\n",
      "          WestJet Crew       1715         13     0.0934      0.538      0.183      0.118\n",
      "         Airport Staff       1715         21       0.31      0.524      0.313      0.249\n",
      "       Air Canada Crew       1715          2     0.0258          1     0.0829     0.0371\n",
      "Speed: 0.2ms preprocess, 117.5ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolov5nu.pt')  # Pretrained model\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data='/Users/Kety/Desktop/archive 12.42.03 PM/data.yaml',  # Path to my data.yaml\n",
    "    epochs=10,  # Number of epochs\n",
    "    imgsz=416,  # Image size (416x416)\n",
    "    batch=8,  # Batch size\n",
    "    device='cpu'  # Use CPU for training;\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('trained_yolov5n_10.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56df03a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'TV', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush', 80: 'Aluminium foil', 81: 'Bottle cap', 82: 'Bottle', 83: 'Broken glass', 84: 'Can', 85: 'Carton', 86: 'Cigarette', 87: 'Cup', 88: 'Lid', 89: 'Other litter', 90: 'Other plastic', 91: 'Paper', 92: 'Plastic bag - wrapper', 93: 'Plastic container', 94: 'Pop tab', 95: 'Straw', 96: 'Styrofoam piece', 97: 'Unlabeled litter', 98: 'WestJet Crew', 99: 'Airport Staff', 100: 'Air Canada Crew'}\n"
     ]
    }
   ],
   "source": [
    "labels105 = model.names\n",
    "print(labels105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f45fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.1.47 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.43 ðŸš€ Python-3.11.5 torch-2.2.1 CPU (Apple M2 Max)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=trained_yolov5n_10.pt, data=/Users/Kety/Desktop/archive 12.42.03 PM/data.yaml, epochs=10, time=None, patience=100, batch=8, imgsz=416, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1760  ultralytics.nn.modules.conv.Conv             [3, 16, 6, 2, 2]              \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      4800  ultralytics.nn.modules.block.C3              [32, 32, 1]                   \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     29184  ultralytics.nn.modules.block.C3              [64, 64, 2]                   \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  3    156928  ultralytics.nn.modules.block.C3              [128, 128, 3]                 \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1]                 \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1     90880  ultralytics.nn.modules.block.C3              [256, 128, 1, False]          \n",
      " 14                  -1  1      8320  ultralytics.nn.modules.conv.Conv             [128, 64, 1, 1]               \n",
      " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     22912  ultralytics.nn.modules.block.C3              [128, 64, 1, False]           \n",
      " 18                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1     74496  ultralytics.nn.modules.block.C3              [128, 128, 1, False]          \n",
      " 21                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 23                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1, False]          \n",
      " 24        [17, 20, 23]  1   1086907  ultralytics.nn.modules.head.Detect           [101, [64, 128, 256]]         \n",
      "YOLOv5n summary: 262 layers, 2844059 parameters, 2844043 gradients, 8.7 GFLOPs\n",
      "\n",
      "Transferred 391/427 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train5', view at http://localhost:6006/\n",
      "Freezing layer 'model.24.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/Kety/Desktop/archive 12.42.03 PM/train/labels.cache... 42\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/Kety/Desktop/archive 12.42.03 PM/valid/labels.cache... 1715\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train5/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=9.5e-05, momentum=0.9) with parameter groups 69 weight(decay=0.0), 76 weight(decay=0.0005), 75 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kety/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train5\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G      1.388       5.15      1.206          1        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866      0.371      0.097     0.0296     0.0208\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G      1.382       3.77      1.177          3        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866      0.205      0.144     0.0599     0.0407\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G      1.336      3.352      1.151          1        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866       0.21      0.225     0.0848     0.0606\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.315      3.141      1.132          3        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866      0.167       0.23     0.0926     0.0672\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G      1.276      2.994      1.116          1        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866      0.179      0.243      0.105     0.0758\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G       1.26      2.885      1.103          4        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866      0.194      0.256      0.123     0.0889\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G      1.255      2.822      1.094          1        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866        0.2      0.257      0.127     0.0908\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G      1.247      2.736      1.089          4        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866       0.21      0.258      0.135     0.0978\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G      1.235      2.688      1.082          2        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866      0.238      0.255      0.141      0.101\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G      1.221      2.659      1.077          3        416: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866      0.237      0.256      0.143      0.104\n",
      "\n",
      "10 epochs completed in 1.605 hours.\n",
      "Optimizer stripped from runs/detect/train5/weights/last.pt, 5.9MB\n",
      "Optimizer stripped from runs/detect/train5/weights/best.pt, 5.9MB\n",
      "\n",
      "Validating runs/detect/train5/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.43 ðŸš€ Python-3.11.5 torch-2.2.1 CPU (Apple M2 Max)\n",
      "YOLOv5n summary (fused): 193 layers, 2838323 parameters, 0 gradients, 8.6 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1715       4866      0.237      0.256      0.144      0.104\n",
      "        Aluminium foil       1715         62      0.397      0.161      0.205      0.161\n",
      "            Bottle cap       1715        459      0.333      0.484      0.336      0.247\n",
      "                Bottle       1715        320      0.379      0.266      0.242       0.17\n",
      "          Broken glass       1715        123          1          0   0.000549   0.000112\n",
      "                   Can       1715        267      0.315      0.509      0.324      0.225\n",
      "                Carton       1715        263      0.248      0.384       0.24      0.179\n",
      "             Cigarette       1715        565          0          0    0.00778    0.00334\n",
      "                   Cup       1715        186      0.185      0.285      0.151       0.11\n",
      "                   Lid       1715         93        0.2      0.199      0.131      0.109\n",
      "          Other litter       1715        178      0.145     0.0674     0.0596     0.0418\n",
      "         Other plastic       1715        265      0.251     0.0377     0.0484     0.0312\n",
      "                 Paper       1715        178     0.0679     0.0127     0.0342     0.0265\n",
      " Plastic bag - wrapper       1715        854      0.308      0.389      0.257      0.181\n",
      "     Plastic container       1715         90      0.232      0.133      0.147        0.1\n",
      "               Pop tab       1715        125          0          0     0.0103    0.00614\n",
      "                 Straw       1715        120      0.154      0.192     0.0922     0.0685\n",
      "       Styrofoam piece       1715        113      0.169      0.168      0.116      0.099\n",
      "      Unlabeled litter       1715        569      0.167     0.0176     0.0338     0.0152\n",
      "          WestJet Crew       1715         13     0.0934      0.538      0.183      0.118\n",
      "         Airport Staff       1715         21       0.31      0.524      0.313      0.249\n",
      "       Air Canada Crew       1715          2     0.0258          1     0.0829     0.0371\n",
      "Speed: 0.3ms preprocess, 120.0ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('trained_yolov5n_10.pt')  # Pretrained model\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data='/Users/Kety/Desktop/archive 12.42.03 PM/data.yaml',  # Path to my data.yaml\n",
    "    epochs=10,  # Number of epochs\n",
    "    imgsz=416,  # Image size (416x416)\n",
    "    batch=8,  # Batch size\n",
    "    device='cpu'  # Use CPU for training;\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('trained_yolov5n_20.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "403e7346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'TV', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush', 80: 'Aluminium foil', 81: 'Bottle cap', 82: 'Bottle', 83: 'Broken glass', 84: 'Can', 85: 'Carton', 86: 'Cigarette', 87: 'Cup', 88: 'Lid', 89: 'Other litter', 90: 'Other plastic', 91: 'Paper', 92: 'Plastic bag - wrapper', 93: 'Plastic container', 94: 'Pop tab', 95: 'Straw', 96: 'Styrofoam piece', 97: 'Unlabeled litter', 98: 'WestJet Crew', 99: 'Airport Staff', 100: 'Air Canada Crew'}\n"
     ]
    }
   ],
   "source": [
    "labels505 = model.names\n",
    "print(labels505)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6686ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation class IDs adjusted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "# Set the path to your annotations directory\n",
    "\n",
    "#annotations_dir = '/Users/Kety/Desktop/Model training/valid/labels'\n",
    "#annotations_dir = '/Users/Kety/Desktop/Model training/test/labels'\n",
    "annotations_dir = '/Users/Kety/Desktop/training_taco_staff/stain/annotations/stain'\n",
    "# Iterate over each file in the annotations directory\n",
    "\n",
    "for filename in os.listdir(annotations_dir):\n",
    "\n",
    "    # Ensure we're only editing .txt files (YOLO annotations)\n",
    "\n",
    "    if filename.endswith('.txt'):\n",
    "\n",
    "        # Build the full path to the annotation file\n",
    "\n",
    "        file_path = os.path.join(annotations_dir, filename)\n",
    "\n",
    "        # Read the original annotations\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Adjust the class IDs in each annotation\n",
    "\n",
    "        adjusted_lines = []\n",
    "\n",
    "        for line in lines:\n",
    "\n",
    "            parts = line.strip().split(' ')\n",
    "\n",
    "            class_id = int(parts[0]) + 101  # subtract 80 to the class ID\n",
    "\n",
    "            adjusted_line = ' '.join([str(class_id)] + parts[1:]) + '\\n'\n",
    "\n",
    "            adjusted_lines.append(adjusted_line)\n",
    "\n",
    "        # Write the adjusted annotations back to the file\n",
    "\n",
    "        with open(file_path, 'w') as file:\n",
    "\n",
    "            file.writelines(adjusted_lines)\n",
    " \n",
    "print(\"Annotation class IDs adjusted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a851c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "# Define paths\n",
    "base_dir = \"/Users/Kety/Desktop/training_taco_staff/stain\"\n",
    "image_dir = os.path.join(base_dir, \"images/stain\")\n",
    "annotation_dir = os.path.join(base_dir, \"annotations/stain\")\n",
    "\n",
    "# Gather file names\n",
    "images = sorted(glob(os.path.join(image_dir, \"*.jpg\")))\n",
    "annotations = sorted(glob(os.path.join(annotation_dir, \"*.txt\")))\n",
    "\n",
    "# Shuffle with the same order\n",
    "combined = list(zip(images, annotations))\n",
    "random.shuffle(combined)\n",
    "images, annotations = zip(*combined)\n",
    "\n",
    "# Split ratio\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(images) * split_ratio)\n",
    "\n",
    "# Training and validation sets\n",
    "train_images, valid_images = images[:split_index], images[split_index:]\n",
    "train_annotations, valid_annotations = annotations[:split_index], annotations[split_index:]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "train_img_dir = os.path.join(base_dir, \"train/images\")\n",
    "train_ann_dir = os.path.join(base_dir, \"train/annotations\")\n",
    "valid_img_dir = os.path.join(base_dir, \"valid/images\")\n",
    "valid_ann_dir = os.path.join(base_dir, \"valid/annotations\")\n",
    "\n",
    "for d in [train_img_dir, train_ann_dir, valid_img_dir, valid_ann_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Function to copy files\n",
    "def copy_files(files, dest_dir):\n",
    "    for f in files:\n",
    "        shutil.copy(f, dest_dir)\n",
    "\n",
    "# Copy files to respective directories\n",
    "copy_files(train_images, train_img_dir)\n",
    "copy_files(train_annotations, train_ann_dir)\n",
    "copy_files(valid_images, valid_img_dir)\n",
    "copy_files(valid_annotations, valid_ann_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec74498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
